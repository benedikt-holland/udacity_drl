{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from IPython import display\n",
    "from datetime import datetime\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "environment = UnityEnvironment(file_name=\"Banana/Banana.exe\", no_graphics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        #self.weights = deque([], maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "        #self.weights.append(args[3])\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    def to(self, device):\n",
    "        for transition in self.memory:\n",
    "            for tensor in transition:\n",
    "                tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, hidden_neurons=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input = nn.Linear(n_observations, hidden_neurons)\n",
    "        self.hidden = nn.Linear(hidden_neurons, hidden_neurons)\n",
    "        #self.hidden2 = nn.Linear(hidden_neurons, hidden_neurons)\n",
    "        self.output = nn.Linear(hidden_neurons, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hidden(x))\n",
    "        #x = F.relu(self.hidden2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, batch_size=128, gamma=0.99, eps_start=0.9, eps_end=0.05, eps_decay=1000, tau=0.005, lr=1e-4, memory_size=10000, hidden_neurons=128, update_every=1):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env = env        \n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "        info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "        state = info.vector_observations[0]\n",
    "        n_actions = self.brain.vector_action_space_size\n",
    "        n_observation = len(state)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.tau = tau\n",
    "        self.update_every = update_every\n",
    "        self.episode_rewards = [] \n",
    "        self.lr = lr\n",
    "        self.policy_net = DQN(n_observation, n_actions, self.hidden_neurons)\n",
    "        self.policy_net.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr, amsgrad=True)\n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        self.steps_done = 0\n",
    "        self.target_net = DQN(n_observation, n_actions, self.hidden_neurons)\n",
    "        self.target_net.to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "       \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1 * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1,1)\n",
    "        else:\n",
    "            return torch.tensor([random.sample(range(0,4), 1)], device=self.device, dtype=torch.long)\n",
    "        \n",
    "    def plot_durations(self, show_result=False):\n",
    "        plt.figure(1)\n",
    "        durations_t = torch.tensor(self.episode_rewards, dtype=torch.float)\n",
    "        if show_result:\n",
    "            plt.title(\"Result\")\n",
    "        else:\n",
    "            plt.clf()\n",
    "            plt.title(\"Training...\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.plot(durations_t.numpy())\n",
    "        if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0,100,1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "        plt.axhline(y=13, color='r')\n",
    "        plt.pause(0.001)\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf());\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf());\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        next_batch = torch.cat(batch.next_state)\n",
    "        #state_action_values = self.policy_net(state_batch).gather(2, action_batch.unsqueeze(1))\n",
    "        state_action_values = torch.max(self.policy_net(state_batch), 2)[0]\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_batch).max(1)[0]\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def train(self, num_episodes=1000):\n",
    "        for _ in range(num_episodes):\n",
    "            info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "            state = info.vector_observations[0]\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            episode_reward = 0\n",
    "            while True:\n",
    "                action = self.select_action(state)\n",
    "                info = self.env.step(int(action))[self.brain_name]\n",
    "                next_state = info.vector_observations[0]\n",
    "                reward = info.rewards[0]\n",
    "                episode_reward += reward\n",
    "                done = info.local_done[0]\n",
    "                reward = torch.tensor([reward], device=self.device)\n",
    "                \n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "                #action = torch.tensor(action, dtype=torch.int64, device=self.device).unsqueeze(0)\n",
    "                self.memory.push(state,action, next_state, reward)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if self.steps_done % self.update_every == 0:\n",
    "                    self.optimize_model()\n",
    "\n",
    "                target_net_state_dict = self.target_net.state_dict()\n",
    "                policy_net_state_dict = self.policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
    "                #if self.steps_done % (1/self.tau) == 0:\n",
    "                #    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "                if done:\n",
    "                    self.episode_rewards.append(episode_reward)\n",
    "                    self.plot_durations()\n",
    "                    break\n",
    "        print('Complete')\n",
    "        self.plot_durations(show_result=True)\n",
    "        plt.ioff()\n",
    "        plt.show();\n",
    "        \n",
    "    def save(self, filename=None):\n",
    "        print(\"Saving...\")\n",
    "        if filename is None:\n",
    "            filename = \"./model_\" + datetime.now().strftime(\"%Y%m%d_%H%M\") + \".pth\"\n",
    "        torch.save({\n",
    "                'policy_net_state_dict': self.policy_net.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'batch_size': self.batch_size,\n",
    "                'gamma': self.gamma,\n",
    "                'eps_start': self.eps_start,\n",
    "                'eps_end': self.eps_end,\n",
    "                'eps_decay': self.eps_decay,\n",
    "                'tau': self.tau,\n",
    "                'episode_rewards': self.episode_rewards,\n",
    "                'memory': self.memory,\n",
    "                'steps_done': self.steps_done,\n",
    "                'lr': self.lr,\n",
    "                'hidden_neurons': self.hidden_neurons\n",
    "            }, filename)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        print(f\"Loading model {filename}\")\n",
    "        config = torch.load(filename, map_location=torch.device(\"cpu\").type)\n",
    "        self.policy_net.load_state_dict(config[\"policy_net_state_dict\"])\n",
    "        self.policy_net.to(self.device)\n",
    "        self.optimizer.load_state_dict(config[\"optimizer_state_dict\"])\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.eps_start = config[\"eps_start\"]\n",
    "        self.eps_end = config[\"eps_end\"]\n",
    "        self.eps_decay = config[\"eps_decay\"]\n",
    "        self.tau = config[\"tau\"]\n",
    "        self.episode_rewards = config[\"episode_rewards\"]\n",
    "        self.memory = config[\"memory\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.memory.to(self.device)\n",
    "        self.steps_done = config[\"steps_done\"]\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3147892135e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-f048848757ff>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps_done\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[0mtarget_net_state_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-f048848757ff>\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mstate_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0maction_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mreward_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mnext_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m#state_action_values = self.policy_net(state_batch).gather(2, action_batch.unsqueeze(1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "filename = \"model1.pth\"\n",
    "agent.load(filename)\n",
    "try:\n",
    "    while True:\n",
    "        agent.train(500)\n",
    "        agent.save(filename)\n",
    "finally:\n",
    "    agent.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = agent.memory.sample(agent.batch_size)\n",
    "batch = Transition(*zip(*transitions))\n",
    "state_batch = torch.cat(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward_batch = torch.cat(batch.reward)\n",
    "next_batch = torch.cat(batch.next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 4])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy_net(state_batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0274]],\n",
       "\n",
       "        [[ 1.8009]],\n",
       "\n",
       "        [[ 1.0672]],\n",
       "\n",
       "        [[ 1.4209]],\n",
       "\n",
       "        [[ 1.6835]],\n",
       "\n",
       "        [[ 2.8674]],\n",
       "\n",
       "        [[ 1.1630]],\n",
       "\n",
       "        [[ 2.4487]],\n",
       "\n",
       "        [[ 2.4569]],\n",
       "\n",
       "        [[ 1.7168]],\n",
       "\n",
       "        [[ 1.3255]],\n",
       "\n",
       "        [[ 1.9042]],\n",
       "\n",
       "        [[ 1.5935]],\n",
       "\n",
       "        [[ 1.3856]],\n",
       "\n",
       "        [[ 2.4793]],\n",
       "\n",
       "        [[ 1.6784]],\n",
       "\n",
       "        [[ 1.5149]],\n",
       "\n",
       "        [[ 1.6144]],\n",
       "\n",
       "        [[ 0.5290]],\n",
       "\n",
       "        [[ 2.6444]],\n",
       "\n",
       "        [[ 2.5171]],\n",
       "\n",
       "        [[ 1.5716]],\n",
       "\n",
       "        [[ 2.4631]],\n",
       "\n",
       "        [[ 2.1324]],\n",
       "\n",
       "        [[ 2.8254]],\n",
       "\n",
       "        [[ 2.3340]],\n",
       "\n",
       "        [[ 1.7705]],\n",
       "\n",
       "        [[ 1.5746]],\n",
       "\n",
       "        [[ 1.8811]],\n",
       "\n",
       "        [[ 1.6946]],\n",
       "\n",
       "        [[ 1.6922]],\n",
       "\n",
       "        [[ 1.6769]],\n",
       "\n",
       "        [[ 1.5648]],\n",
       "\n",
       "        [[ 0.3841]],\n",
       "\n",
       "        [[ 1.6346]],\n",
       "\n",
       "        [[ 2.6931]],\n",
       "\n",
       "        [[-1.1712]],\n",
       "\n",
       "        [[ 1.9579]],\n",
       "\n",
       "        [[ 1.5058]],\n",
       "\n",
       "        [[ 1.3613]],\n",
       "\n",
       "        [[ 1.4581]],\n",
       "\n",
       "        [[ 1.3593]],\n",
       "\n",
       "        [[ 1.3963]],\n",
       "\n",
       "        [[ 1.6367]],\n",
       "\n",
       "        [[ 1.6922]],\n",
       "\n",
       "        [[ 2.5898]],\n",
       "\n",
       "        [[ 1.6489]],\n",
       "\n",
       "        [[ 2.6144]],\n",
       "\n",
       "        [[ 1.7639]],\n",
       "\n",
       "        [[ 1.4924]],\n",
       "\n",
       "        [[ 1.4209]],\n",
       "\n",
       "        [[ 0.3743]],\n",
       "\n",
       "        [[ 1.3093]],\n",
       "\n",
       "        [[ 1.7808]],\n",
       "\n",
       "        [[ 1.6966]],\n",
       "\n",
       "        [[ 1.7143]],\n",
       "\n",
       "        [[ 1.4536]],\n",
       "\n",
       "        [[ 1.6061]],\n",
       "\n",
       "        [[ 1.6960]],\n",
       "\n",
       "        [[ 1.3729]],\n",
       "\n",
       "        [[ 1.4954]],\n",
       "\n",
       "        [[ 1.8180]],\n",
       "\n",
       "        [[ 2.1144]],\n",
       "\n",
       "        [[-0.5416]],\n",
       "\n",
       "        [[ 1.7352]],\n",
       "\n",
       "        [[ 1.2940]],\n",
       "\n",
       "        [[ 2.2955]],\n",
       "\n",
       "        [[ 1.5187]],\n",
       "\n",
       "        [[ 1.5998]],\n",
       "\n",
       "        [[ 2.0178]],\n",
       "\n",
       "        [[ 2.0340]],\n",
       "\n",
       "        [[ 2.1797]],\n",
       "\n",
       "        [[ 2.1328]],\n",
       "\n",
       "        [[ 1.6159]],\n",
       "\n",
       "        [[-1.6161]],\n",
       "\n",
       "        [[ 2.0551]],\n",
       "\n",
       "        [[ 1.7278]],\n",
       "\n",
       "        [[ 2.4722]],\n",
       "\n",
       "        [[ 1.5639]],\n",
       "\n",
       "        [[ 1.6679]],\n",
       "\n",
       "        [[ 1.4144]],\n",
       "\n",
       "        [[ 1.4800]],\n",
       "\n",
       "        [[ 1.7112]],\n",
       "\n",
       "        [[-0.9777]],\n",
       "\n",
       "        [[ 1.3719]],\n",
       "\n",
       "        [[ 1.9769]],\n",
       "\n",
       "        [[ 1.8235]],\n",
       "\n",
       "        [[ 1.5315]],\n",
       "\n",
       "        [[ 1.7977]],\n",
       "\n",
       "        [[ 2.5333]],\n",
       "\n",
       "        [[ 1.7781]],\n",
       "\n",
       "        [[ 1.6142]],\n",
       "\n",
       "        [[ 1.5843]],\n",
       "\n",
       "        [[ 1.5529]],\n",
       "\n",
       "        [[ 1.4446]],\n",
       "\n",
       "        [[ 2.3842]],\n",
       "\n",
       "        [[ 1.6483]],\n",
       "\n",
       "        [[ 1.2123]],\n",
       "\n",
       "        [[ 1.5652]],\n",
       "\n",
       "        [[ 2.4788]],\n",
       "\n",
       "        [[ 1.7018]],\n",
       "\n",
       "        [[ 1.3988]],\n",
       "\n",
       "        [[-1.3348]],\n",
       "\n",
       "        [[ 2.7894]],\n",
       "\n",
       "        [[ 1.5784]],\n",
       "\n",
       "        [[ 1.5077]],\n",
       "\n",
       "        [[ 1.6446]],\n",
       "\n",
       "        [[ 1.5998]],\n",
       "\n",
       "        [[ 2.0987]],\n",
       "\n",
       "        [[-0.1903]],\n",
       "\n",
       "        [[ 2.5818]],\n",
       "\n",
       "        [[ 2.2764]],\n",
       "\n",
       "        [[ 2.6729]],\n",
       "\n",
       "        [[ 1.6036]],\n",
       "\n",
       "        [[ 1.8500]],\n",
       "\n",
       "        [[ 1.7032]],\n",
       "\n",
       "        [[ 1.0961]],\n",
       "\n",
       "        [[ 1.0589]],\n",
       "\n",
       "        [[ 1.6990]],\n",
       "\n",
       "        [[ 2.0129]],\n",
       "\n",
       "        [[ 1.4346]],\n",
       "\n",
       "        [[ 2.2910]],\n",
       "\n",
       "        [[ 1.9078]],\n",
       "\n",
       "        [[ 2.8288]],\n",
       "\n",
       "        [[ 1.4474]],\n",
       "\n",
       "        [[ 2.3895]],\n",
       "\n",
       "        [[ 1.4049]],\n",
       "\n",
       "        [[ 1.5680]]], grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy_net(state_batch).gather(2, action_batch.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0274e+00,  2.8013e-01,  1.4149e+00, -1.0236e+00]],\n",
       "\n",
       "        [[ 1.5846e+00,  1.4660e+00,  1.8009e+00, -1.0691e+00]],\n",
       "\n",
       "        [[ 1.0672e+00,  1.0091e+00, -6.1317e-01, -4.1920e-01]],\n",
       "\n",
       "        [[ 1.5027e+00,  1.4209e+00,  5.0773e-01, -7.8916e-01]],\n",
       "\n",
       "        [[ 1.6835e+00,  1.2602e+00,  1.5284e+00, -6.6846e-01]],\n",
       "\n",
       "        [[ 2.8674e+00, -1.5503e-01,  1.7513e+00, -1.4482e+00]],\n",
       "\n",
       "        [[ 1.7720e+00,  1.1630e+00,  8.9331e-01, -9.0329e-01]],\n",
       "\n",
       "        [[ 2.4487e+00, -1.0330e+00, -1.2747e+00, -1.3621e+00]],\n",
       "\n",
       "        [[ 2.4569e+00, -3.5724e-01, -5.6432e-01, -1.1987e+00]],\n",
       "\n",
       "        [[ 1.7168e+00,  2.2933e-03, -2.2989e-01, -1.1900e+00]],\n",
       "\n",
       "        [[ 1.3218e+00,  1.3255e+00,  1.0797e+00, -3.9888e-01]],\n",
       "\n",
       "        [[ 1.6237e+00,  1.3657e+00,  1.9042e+00, -7.1986e-01]],\n",
       "\n",
       "        [[ 1.4166e+00,  1.5935e+00,  1.0443e+00, -2.7608e-01]],\n",
       "\n",
       "        [[ 1.3856e+00,  1.3105e+00,  1.0963e+00, -2.8390e-01]],\n",
       "\n",
       "        [[ 2.4793e+00, -2.3651e+00, -1.6910e+00, -1.0670e+00]],\n",
       "\n",
       "        [[ 1.6784e+00,  1.3811e+00,  1.1934e+00, -1.2406e+00]],\n",
       "\n",
       "        [[ 1.5149e+00,  1.4653e+00, -4.6254e-01, -4.4924e-01]],\n",
       "\n",
       "        [[ 1.6144e+00,  1.3763e+00,  1.3674e+00, -5.6609e-01]],\n",
       "\n",
       "        [[ 1.7065e+00,  5.2897e-01,  1.6497e+00, -4.8417e-01]],\n",
       "\n",
       "        [[ 2.6444e+00, -6.5708e-01,  1.1476e-01, -1.1954e+00]],\n",
       "\n",
       "        [[ 2.5171e+00,  1.6474e+00,  2.3585e+00, -6.5654e-01]],\n",
       "\n",
       "        [[ 1.3020e+00,  1.5716e+00, -3.3524e-01, -2.8804e-01]],\n",
       "\n",
       "        [[ 2.4631e+00,  5.2558e-01,  2.0901e+00, -9.9025e-01]],\n",
       "\n",
       "        [[ 2.1324e+00,  1.5935e+00,  2.0292e+00, -8.6787e-01]],\n",
       "\n",
       "        [[ 2.8254e+00, -6.2774e-01,  6.7265e-01, -1.3254e+00]],\n",
       "\n",
       "        [[ 2.3340e+00,  4.7083e-01,  1.5926e+00, -3.2321e-01]],\n",
       "\n",
       "        [[ 1.7705e+00,  1.5390e+00,  5.7368e-01, -4.4979e-01]],\n",
       "\n",
       "        [[ 1.5746e+00,  1.4298e+00,  1.0260e+00, -1.0806e+00]],\n",
       "\n",
       "        [[ 1.8751e+00,  1.5348e+00,  1.8811e+00, -5.5929e-01]],\n",
       "\n",
       "        [[ 1.6946e+00,  1.5546e+00,  4.4730e-01, -9.3424e-01]],\n",
       "\n",
       "        [[ 1.6861e+00,  1.6922e+00,  8.7530e-01, -6.8930e-01]],\n",
       "\n",
       "        [[ 1.6769e+00,  1.4630e+00,  6.3366e-01, -8.3352e-01]],\n",
       "\n",
       "        [[ 1.5648e+00,  1.4717e+00,  1.1704e+00, -1.0665e+00]],\n",
       "\n",
       "        [[ 1.4496e+00,  1.2950e+00,  3.8407e-01, -4.7974e-01]],\n",
       "\n",
       "        [[ 1.6346e+00,  1.5530e+00,  1.1230e+00, -1.0073e+00]],\n",
       "\n",
       "        [[ 2.6931e+00, -1.5203e+00, -6.2381e-03, -1.0666e+00]],\n",
       "\n",
       "        [[ 2.7968e+00, -3.5747e-01,  1.9500e+00, -1.1712e+00]],\n",
       "\n",
       "        [[ 1.9579e+00,  1.3757e+00,  1.4805e+00, -1.0766e+00]],\n",
       "\n",
       "        [[ 1.5058e+00,  1.3193e+00, -6.4596e-01, -4.4498e-01]],\n",
       "\n",
       "        [[ 1.4305e+00,  1.3613e+00, -4.9382e-01, -3.8316e-01]],\n",
       "\n",
       "        [[ 1.4581e+00,  1.3783e+00,  6.1287e-01, -6.3954e-01]],\n",
       "\n",
       "        [[ 1.3527e+00,  1.3593e+00,  1.1942e+00, -3.7111e-01]],\n",
       "\n",
       "        [[ 1.4040e+00,  1.3963e+00, -3.8401e-01, -3.6344e-01]],\n",
       "\n",
       "        [[ 1.6367e+00,  1.6953e+00,  1.3224e+00, -4.0935e-01]],\n",
       "\n",
       "        [[ 1.6922e+00,  1.3468e+00,  1.2734e+00, -8.7361e-01]],\n",
       "\n",
       "        [[ 2.5898e+00, -1.2878e+00,  3.3851e-01, -1.2911e+00]],\n",
       "\n",
       "        [[ 1.6807e+00,  1.6489e+00,  6.2736e-01, -2.2073e-01]],\n",
       "\n",
       "        [[ 2.6144e+00, -6.0374e-01,  1.0867e+00, -5.1236e-01]],\n",
       "\n",
       "        [[ 1.7639e+00,  1.3346e+00,  5.5552e-01, -7.8023e-01]],\n",
       "\n",
       "        [[ 1.4924e+00,  1.3916e+00,  8.9023e-01, -2.0725e-01]],\n",
       "\n",
       "        [[ 1.7178e+00,  1.4209e+00,  9.3925e-01, -8.7717e-01]],\n",
       "\n",
       "        [[ 1.3432e+00,  1.2772e+00,  3.7428e-01, -6.2893e-02]],\n",
       "\n",
       "        [[ 1.3093e+00,  1.2044e+00, -8.3365e-02, -6.2100e-01]],\n",
       "\n",
       "        [[ 1.4591e+00,  1.7808e+00,  1.5484e+00, -4.9796e-01]],\n",
       "\n",
       "        [[ 1.6966e+00,  1.3788e+00,  6.1400e-01, -9.6079e-01]],\n",
       "\n",
       "        [[ 1.7143e+00,  1.4064e+00,  1.5760e+00, -3.7792e-01]],\n",
       "\n",
       "        [[ 1.4089e+00,  1.4536e+00,  1.2098e-01, -5.7216e-01]],\n",
       "\n",
       "        [[ 1.6061e+00,  1.4136e+00,  1.1397e+00, -1.1375e+00]],\n",
       "\n",
       "        [[ 1.6960e+00,  1.1684e+00,  1.3815e+00, -7.8856e-01]],\n",
       "\n",
       "        [[ 1.4857e+00,  1.3729e+00,  7.7053e-01, -6.3976e-01]],\n",
       "\n",
       "        [[ 1.3960e+00,  1.4954e+00,  5.5320e-01, -5.0671e-01]],\n",
       "\n",
       "        [[ 1.8180e+00,  1.4534e+00,  5.2787e-01, -9.6186e-01]],\n",
       "\n",
       "        [[ 2.1144e+00,  1.1288e+00,  2.2813e+00, -8.6055e-01]],\n",
       "\n",
       "        [[ 1.5522e+00,  1.3872e+00,  1.4809e+00, -5.4160e-01]],\n",
       "\n",
       "        [[ 1.9423e+00,  1.5433e+00,  1.7352e+00, -8.2423e-01]],\n",
       "\n",
       "        [[ 1.3432e+00,  1.2940e+00,  5.6499e-01, -9.3411e-01]],\n",
       "\n",
       "        [[ 2.2955e+00, -5.8833e-01, -2.9414e-02, -1.0877e+00]],\n",
       "\n",
       "        [[ 1.5187e+00,  1.3373e+00,  7.1187e-01, -1.0558e+00]],\n",
       "\n",
       "        [[ 1.5998e+00,  1.5680e+00, -3.4852e-01, -6.0719e-01]],\n",
       "\n",
       "        [[ 2.0178e+00,  6.5784e-01,  4.3505e-01, -7.8689e-01]],\n",
       "\n",
       "        [[ 2.0340e+00,  7.7906e-02,  2.4506e-01, -1.1038e+00]],\n",
       "\n",
       "        [[ 1.6656e+00,  1.5154e+00,  2.1797e+00, -4.5952e-01]],\n",
       "\n",
       "        [[ 2.1328e+00,  1.3377e+00,  1.8170e+00, -8.2068e-01]],\n",
       "\n",
       "        [[ 1.3289e+00,  1.6159e+00,  9.2349e-01, -3.8144e-01]],\n",
       "\n",
       "        [[ 2.4585e+00, -1.6161e+00,  1.6557e-01, -1.0720e+00]],\n",
       "\n",
       "        [[ 2.0551e+00, -1.9164e-01,  7.7384e-01, -1.1915e+00]],\n",
       "\n",
       "        [[ 1.5864e+00,  1.7278e+00,  1.0118e+00, -5.2608e-01]],\n",
       "\n",
       "        [[ 2.1723e+00,  3.0937e-01,  2.4722e+00, -1.0452e+00]],\n",
       "\n",
       "        [[ 1.4979e+00,  1.5639e+00,  1.2264e+00, -2.5844e-01]],\n",
       "\n",
       "        [[ 1.6679e+00,  1.2806e+00,  1.5460e+00, -8.1942e-01]],\n",
       "\n",
       "        [[ 1.4374e+00,  1.3610e+00,  1.4144e+00, -4.0760e-01]],\n",
       "\n",
       "        [[ 1.4800e+00,  1.4697e+00,  8.8126e-01, -2.0096e-01]],\n",
       "\n",
       "        [[ 1.7112e+00,  1.0294e+00,  1.0414e+00, -7.6780e-01]],\n",
       "\n",
       "        [[ 1.7431e+00,  4.4847e-01, -5.3959e-02, -9.7769e-01]],\n",
       "\n",
       "        [[ 1.3719e+00,  1.2950e+00,  5.3343e-01, -2.2892e-01]],\n",
       "\n",
       "        [[ 1.8318e+00,  1.3844e+00,  1.9769e+00, -1.0615e+00]],\n",
       "\n",
       "        [[ 1.8235e+00,  7.0268e-01,  1.3833e+00, -9.0289e-01]],\n",
       "\n",
       "        [[ 1.5409e+00,  1.5315e+00,  3.5507e-01, -7.6802e-01]],\n",
       "\n",
       "        [[ 1.7977e+00,  1.5228e+00,  1.0078e+00, -9.7282e-01]],\n",
       "\n",
       "        [[ 2.5333e+00, -1.3637e+00,  3.1200e-02, -5.7092e-01]],\n",
       "\n",
       "        [[ 1.7781e+00,  1.6887e+00,  1.2302e+00, -4.0110e-01]],\n",
       "\n",
       "        [[ 1.6142e+00,  1.4789e+00,  1.0928e+00, -1.1733e-01]],\n",
       "\n",
       "        [[ 1.5843e+00,  1.3314e+00,  1.3239e+00, -5.2402e-01]],\n",
       "\n",
       "        [[ 1.5642e+00,  1.5889e+00,  1.5529e+00, -1.9074e-01]],\n",
       "\n",
       "        [[ 1.3864e+00,  1.4446e+00, -1.4394e+00, -3.0431e-01]],\n",
       "\n",
       "        [[ 2.3842e+00,  6.2485e-01,  2.0896e+00, -1.1719e+00]],\n",
       "\n",
       "        [[ 1.6483e+00,  1.2405e+00,  1.4711e+00, -6.9791e-01]],\n",
       "\n",
       "        [[ 1.2123e+00,  1.1327e+00,  1.7728e-01, -2.5604e-01]],\n",
       "\n",
       "        [[ 1.5652e+00,  1.3933e+00,  1.2277e+00, -3.2060e-01]],\n",
       "\n",
       "        [[ 2.1781e+00,  1.1877e+00,  2.4788e+00, -8.4258e-01]],\n",
       "\n",
       "        [[ 1.7018e+00,  1.2640e+00,  3.7004e-01, -6.9530e-01]],\n",
       "\n",
       "        [[ 1.3988e+00,  1.2012e+00,  8.4962e-01, -5.4236e-01]],\n",
       "\n",
       "        [[ 1.7640e+00,  5.0330e-01, -6.5087e-02, -1.3348e+00]],\n",
       "\n",
       "        [[ 2.7894e+00, -1.5297e+00,  7.7169e-02, -1.3281e+00]],\n",
       "\n",
       "        [[ 1.5643e+00,  1.5784e+00,  1.1821e+00, -2.8317e-01]],\n",
       "\n",
       "        [[ 1.3999e+00,  1.5077e+00,  1.2131e+00, -2.4717e-01]],\n",
       "\n",
       "        [[ 1.6446e+00,  1.4423e+00,  9.9194e-01, -1.1112e+00]],\n",
       "\n",
       "        [[ 1.5998e+00,  1.5680e+00, -3.4849e-01, -6.0719e-01]],\n",
       "\n",
       "        [[ 2.0987e+00,  1.1066e+00,  1.7270e+00, -1.1808e+00]],\n",
       "\n",
       "        [[ 2.7488e+00, -1.2832e+00, -1.9032e-01, -1.3427e+00]],\n",
       "\n",
       "        [[ 2.5818e+00, -7.7601e-01, -1.6732e-01, -1.3891e+00]],\n",
       "\n",
       "        [[ 2.3866e+00,  2.2764e+00,  2.0044e+00, -1.5976e-01]],\n",
       "\n",
       "        [[ 2.0122e+00,  2.1481e+00,  2.6729e+00, -1.9798e-01]],\n",
       "\n",
       "        [[ 1.6036e+00,  1.4883e+00,  9.0911e-02, -7.3372e-01]],\n",
       "\n",
       "        [[ 1.8500e+00,  7.0144e-01,  6.3384e-01, -7.4146e-01]],\n",
       "\n",
       "        [[ 1.7032e+00,  1.1344e+00,  1.2012e+00, -6.7116e-01]],\n",
       "\n",
       "        [[ 1.0961e+00,  1.0366e+00, -5.6784e-01, -4.4550e-01]],\n",
       "\n",
       "        [[ 1.6206e+00,  1.2869e+00,  1.0589e+00, -7.8553e-01]],\n",
       "\n",
       "        [[ 1.6990e+00,  1.4026e+00,  1.2315e+00, -7.9889e-01]],\n",
       "\n",
       "        [[ 1.9931e+00,  1.7704e+00,  2.0129e+00, -3.4739e-01]],\n",
       "\n",
       "        [[ 1.2924e+00,  1.4346e+00,  6.9353e-01, -4.0640e-01]],\n",
       "\n",
       "        [[ 2.2910e+00,  1.7403e+00,  2.1357e+00, -8.1227e-01]],\n",
       "\n",
       "        [[ 1.9078e+00,  4.0560e-01,  1.1430e+00, -8.6093e-01]],\n",
       "\n",
       "        [[ 2.2361e+00,  1.9522e+00,  2.8288e+00, -4.7906e-01]],\n",
       "\n",
       "        [[ 1.4474e+00,  1.3164e+00, -1.2014e+00, -3.3461e-01]],\n",
       "\n",
       "        [[ 2.3895e+00, -4.1591e-01, -1.6206e-01, -1.0719e+00]],\n",
       "\n",
       "        [[ 1.3452e+00,  1.4049e+00,  2.3349e-01, -3.2898e-01]],\n",
       "\n",
       "        [[ 1.5998e+00,  1.5680e+00, -3.4853e-01, -6.0719e-01]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy_net(state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [0],\n",
       "        [3],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [0],\n",
       "        [3],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [0],\n",
       "        [3],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-0e78d0e6c9f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "agent.policy_net(state_batch).gather(2, action_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details(filename):\n",
    "    config = torch.load(filename, map_location=torch.device(\"cpu\"))\n",
    "    agent.episode_rewards = config[\"episode_rewards\"]\n",
    "    agent.plot_durations(show_result=True)\n",
    "    del config[\"policy_net_state_dict\"]\n",
    "    del config[\"optimizer_state_dict\"]\n",
    "    del config[\"memory\"]\n",
    "    del config[\"episode_rewards\"]\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity",
   "language": "python",
   "name": "udacity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "35cf814d8ed11f2e4f1874cd60f92ea9206450e3b63de2b535f10183330b7e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
