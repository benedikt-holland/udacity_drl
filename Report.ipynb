{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Algorithm\n",
    "For the algorithm a standard DQN was chosen with a seperate target network and replay memory according to the original [DQN paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). Additionally this algorithm implemented batch learning. The official [pytorch documentation](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) was chosen as a tutorial and adjusted to this environment.\n",
    "## Neural Network\n",
    "The neural network used has an input layer of size 37 for the 37 observations the environment returns, an output layer of size 4 for the 4 available actions and a one or multiple variable sized hidden layers.\n",
    "### Hidden layers\n",
    "The task should not require more than one or two hidden layers. Both one and two layer neural networks achieved an average reward of +10 over 100 consecutive episodes, however the two layer network only achieved these performances after 1000 episodes while the one layer only took 200. Additionally the two layer network had a disk size of 1MB while the one layer network only requires 20KB. So in conclusion the single layer network is preferable.\n",
    "### Number of neurons\n",
    "The standard number of neurons choosen for the hidden layer was 128, a lower number of neurons seemed to have negative effects on the agents performance.\n",
    "## Agent\n",
    "### batch size\n",
    "The batch size describes how many experiences the agent will pull out of memory to learn from at a time. Organizing the learning in batches ensures that the agent learns from a batch of experiences that represent the environment as whole instead of a single experience that might be highly unlikely in general. A batch size of 132 was chosen for most attempts.\n",
    "### gamma\n",
    "Gamma is the discount rate and indicates how strongly the agent cares about the future. As this is a task with dense reward a smaller gamma should suffice, because the agent can usually reach a reward with a few actions. Both 0.9 and 0.99 were tested.\n",
    "### epsilon\n",
    "Without random exploration the agent would become too scared of blue bananas and instead of moving turn in circles. To encourage it to find the reward the yellow bananas give random exploration is needed. A small put constant value seems to be perfect for this environment.\n",
    "### epsilon decay\n",
    "Epsilon decay is useful in many environments where the agent needs to initially explore the environment to find an optimal policy. However in this case the environment states are not highly dependent on each other, so initial exploration is not needed. Testing showed that epsilon decay was counterproductive in the beginning and the agent only really started to learn once epsilon decayed to it's final value. See [model 1](#model1).\n",
    "\n",
    "In the beginning of this run the agent didn't manage to score any points, however once epsilon decayed it was able to quickly learn. Because of these insights epsilon decay was removed from future runs.\n",
    "### tau\n",
    "Tau describes the rate at which the target network converges towards the policy network. A value of 0.005 was chosen.\n",
    "### learning rate\n",
    "The learning rate describes how much the agent will adjust its policy in order to fit to the new experiences. Setting it too high will result in the agent locking in on one strategy too quickly without considering more optimal policies or even being to scared of blue bananas to avoid punishment. Setting the learning rate too low will result in a longer training period. Learning rates of both 0.001 and 0.0001 were tested.\n",
    "### memory size\n",
    "Setting the memory size too low resulted in the agent eventually forgetting the most basic things he learnt in the beginning, like avoiding blue bananas. The initial size of 10000 led to decreased performance in later episodes. A memory size of 1000000 seemed more ideal. See [model 2](#model2)\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "### Model 1\n",
    "<a id=\"model1\"></a> \n",
    "\n",
    "Learning rate 0.0001 and epsilon decay 1000\n",
    "\n",
    "<img src=\"images/eps_decay.JPG\">\n",
    "\n",
    "### Model 2\n",
    "<a id=\"model2\"></a> \n",
    "Learning rate 0.001 and epsilon decay 10000\n",
    "Tried: \n",
    "- Higher epsilon end of 0.2\n",
    "\n",
    "Retrained 5 times with different settings from a snapshot taken at 1500 episodes\n",
    "\n",
    "<img src=\"images/memory_size.JPG\">\n",
    "\n",
    "### Parameters\n",
    "### Training\n",
    "### Future work\n",
    "There are many potential improvements to the standard DQN algorithm that could be applied to this problem. Specificially the prioritized replay approach could be beneficial in this scenario as the agent experiences many interactions that are of little value for learning. Like walking from A to B without collecting banana or walking against the wall. As the [Rainbow](https://arxiv.org/abs/1710.02298) paper shows prioritized experience replay together with multi-step bootstrap targets provide the best improvements to the standard DQN algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
