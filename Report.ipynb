{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Algorithm\n",
    "For the algorithm a standard DQN was chosen with a seperate target network and replay memory according to the original [DQN paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). Additionally this algorithm implemented batch learning. The official [pytorch documentation](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) was chosen as a tutorial and adjusted to this environment.\n",
    "## Neural Network\n",
    "The neural network used has an input layer of size 37 for the 37 observations the environment returns, an output layer of size 4 for the 4 available actions and a one or multiple variable sized hidden layers.\n",
    "### Hidden layers\n",
    "The task should not require more than one or two hidden layers. In fact a single layer seemed to be more optimal as any two layer attempts did not produce any results within 2000 episodes while the single layer approaches of [model 1](#model1) and [model 2](#model2) produced results within 1000 episodes. Additionally the two layer networks had a disk size of 0.3 - 1MB while the one layer networks only required about 20KB. So in conclusion the single layer network is preferable.\n",
    "\n",
    "<img src=\"images/2_layers4.JPG\">\n",
    "\n",
    "### Number of neurons\n",
    "The standard number of neurons choosen for the hidden layer was 128, a lower number of neurons seemed to have negative effects on the agents performance. Double hidden layer networks with the number of hidden neurons ranging from 16, 32 to 64 where tested, but didn't achieve any results.\n",
    "## Agent\n",
    "### batch size\n",
    "The batch size describes how many experiences the agent will pull out of memory to learn from at a time. Organizing the learning in batches ensures that the agent learns from a batch of experiences that represent the environment as whole instead of a single experience that might be highly unlikely in general. A batch size of 132 was chosen for most attempts.\n",
    "### gamma\n",
    "Gamma is the discount rate and indicates how strongly the agent cares about the future. As this is a task with dense reward a smaller gamma should suffice, because the agent can usually reach a reward with a few actions. The successful agents had a gamma value of 0.99. 0.9 was tested however it didn't achieve any results.\n",
    "### epsilon\n",
    "Without random exploration the agent would become too scared of blue bananas and instead of moving turn in circles. To encourage it to find the reward the yellow bananas give random exploration is needed. A small put constant value seems to be perfect for this environment. A value of 0.1 was chosen for the models.\n",
    "### epsilon decay\n",
    "Epsilon decay is useful in many environments where the agent needs to initially explore the environment to find an optimal policy. However in this case the environment states are not highly dependent on each other, so initial exploration is not needed. Testing showed that epsilon decay was counterproductive in the beginning and the agent only really started to learn once epsilon decayed to it's final value. In [model 1](#model1) a low epsilon decay value of 1000 was chosen, so exploration stopped after a couple hundred episodes, leading to the agent never learning a more optimal policy. in [model 2](#model2) a high epsilon decay of 10000 was chosen. In this run the initial performance was worsened by the high epsilon value and the agent was only able to learn a good policy after 1000 episodes.\n",
    "\n",
    "<a id=\"model1\"></a> \n",
    "\n",
    "<img src=\"images/eps_decay.JPG\">\n",
    "\n",
    "### tau\n",
    "Tau describes the rate at which the target network converges towards the policy network. A value of 0.005 was chosen.\n",
    "### learning rate\n",
    "The learning rate describes how much the agent will adjust its policy in order to fit to the new experiences. Setting it too high will result in the agent locking in on one strategy too quickly without considering more optimal policies or even being to scared of blue bananas to avoid punishment. Setting the learning rate too low will result in a longer training period. Learning rates of both 0.001 and 0.0001 were tested. [model 1](#model1) used the low learning rate of 0.0001 and [model 2](#model2) the high learning rate of 0.001, however due to the effects of the epsilon parameters the difference can not be distinguished.\n",
    "\n",
    "\n",
    "<a id=\"model2\"></a> \n",
    "\n",
    "<img src=\"images/memory_size.JPG\">\n",
    "\n",
    "### memory size\n",
    "Setting the memory size too low resulted in the agent eventually forgetting the most basic things he learnt in the beginning, like avoiding blue bananas. The initial size of 10000 led to decreased performance in later episodes. See [model 1](#model1). A memory size of 1000000 seemed more ideal. See [model 2](#model2)\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "The final model had the following parameters.\n",
    "\n",
    "<img src=\"images/final.JPG\">\n",
    "\n",
    "### Parameters\n",
    "Model 1: \n",
    "- batch_size: 128\n",
    "- gamma: 0.99\n",
    "- eps_start: 0.9\n",
    "- eps_end: 0.1\n",
    "- eps_decay: 1000\n",
    "- tau: 0.005\n",
    "- lr: 0.0001\n",
    "\n",
    "### Future work\n",
    "There are many potential improvements to the standard DQN algorithm that could be applied to this problem. Specificially the prioritized replay approach could be beneficial in this scenario as the agent experiences many interactions that are of little value for learning. Like walking from A to B without collecting banana or walking against the wall. As the [Rainbow](https://arxiv.org/abs/1710.02298) paper shows prioritized experience replay together with multi-step bootstrap targets provide the best improvements to the standard DQN algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
